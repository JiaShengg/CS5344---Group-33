# ensemble_model_feature_engineering.py
# Systematic feature engineering to beat AP = 0.53335

import pandas as pd
import numpy as np
from scipy import stats
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import average_precision_score
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
import warnings
import json
from datetime import datetime

try:
    from pyod.models.ecod import ECOD
    from pyod.models.copod import COPOD
    from pyod.models.hbos import HBOS
    from pyod.models.knn import KNN
    from pyod.models.inne import INNE
    from pyod.models.cblof import CBLOF
    from pyod.models.loda import LODA
    from pyod.models.cof import COF
    PYOD_AVAILABLE = True
except ImportError:
    PYOD_AVAILABLE = False

warnings.filterwarnings('ignore')

def load_data():
    train_df = pd.read_csv('/home/ubuntu/DS/Track2/loans_train.csv')
    valid_df = pd.read_csv('/home/ubuntu/DS/Track2/loans_valid.csv')
    test_df = pd.read_csv('/home/ubuntu/DS/Track2/loans_test.csv')
    return train_df, valid_df, test_df

def calculate_monthly_payment(principal, annual_rate, term_months):
    monthly_rate = annual_rate / 100 / 12
    if monthly_rate == 0:
        return principal / term_months
    return principal * monthly_rate * (1 + monthly_rate) ** term_months / ((1 + monthly_rate) ** term_months - 1)

# ============================================================================
# FEATURE GROUP 1: BASELINE PAYMENT RATIOS (from original script)
# ============================================================================
def create_baseline_payment_features(df):
    """Original payment ratio features that achieved AP=0.418 - COMPLETE VERSION"""
    features = pd.DataFrame(index=df.index)
    
    # Calculate payment ratios
    for period in range(1, 14):
        current_upb_col = f"{period}_CurrentActualUPB"
        prev_upb_col = f"{period-1}_CurrentActualUPB"
        
        actual_payment = df[prev_upb_col] - df[current_upb_col]
        actual_payment = actual_payment.clip(lower=0)
        
        scheduled_payment = df.apply(
            lambda x: calculate_monthly_payment(
                x['OriginalUPB'], 
                x['OriginalInterestRate'], 
                x['OriginalLoanTerm']
            ), 
            axis=1
        )
        
        ratio = actual_payment / scheduled_payment.replace(0, np.nan)
        upper_bound = ratio.quantile(0.99)
        ratio = ratio.clip(upper=upper_bound)
        features[f"payment_ratio_{period}"] = ratio
    
    # Windowed features (from original script)
    for i in range(3, 14):
        features[f"ratio_mean_win3_{i}"] = features[[f"payment_ratio_{i-2}", 
                                                      f"payment_ratio_{i-1}", 
                                                      f"payment_ratio_{i}"]].mean(axis=1)
        
        features[f"ratio_std_win3_{i}"] = features[[f"payment_ratio_{i-2}", 
                                                     f"payment_ratio_{i-1}", 
                                                     f"payment_ratio_{i}"]].std(axis=1)
        
        features[f"ratio_trend_win3_{i}"] = (features[f"payment_ratio_{i}"] - 
                                             features[f"payment_ratio_{i-2}"]) / 2
    
    # Aggregations
    ratio_cols = [col for col in features.columns if col.startswith('payment_ratio_')]
    features['avg_payment_ratio'] = features[ratio_cols].mean(axis=1)
    features['std_payment_ratio'] = features[ratio_cols].std(axis=1)
    features['min_payment_ratio'] = features[ratio_cols].min(axis=1)
    features['max_payment_ratio'] = features[ratio_cols].max(axis=1)
    features['recent3_avg_ratio'] = features[[f"payment_ratio_{i}" for i in range(11, 14)]].mean(axis=1)
    features['recent5_avg_ratio'] = features[[f"payment_ratio_{i}" for i in range(9, 14)]].mean(axis=1)
    
    # Ratio differences
    for i in range(2, 14):
        features[f"ratio_diff_{i}"] = features[f"payment_ratio_{i}"] - features[f"payment_ratio_{i-1}"]
    
    # Stress signals
    features['under_payment_count'] = (features[ratio_cols] < 1.0).sum(axis=1)
    features['zero_payment_count'] = (features[ratio_cols] == 0).sum(axis=1)
    features['paid_off_flag'] = (df['13_CurrentActualUPB'] == 0).astype(int)
    
    # Payment acceleration
    early_avg = features[[f"payment_ratio_{i}" for i in range(1, 7)]].mean(axis=1)
    late_avg = features[[f"payment_ratio_{i}" for i in range(7, 14)]].mean(axis=1)
    features['payment_acceleration'] = late_avg - early_avg
    
    return features

# ============================================================================
# FEATURE GROUP 2: PAYMENT VELOCITY & ACCELERATION
# Rationale: Loans that are paid down faster are lower risk
# ============================================================================
def create_payment_velocity_features(df):
    """Track how fast the loan balance is decreasing"""
    features = pd.DataFrame(index=df.index)
    
    # Calculate actual principal reduction per month
    for period in range(1, 14):
        reduction = df[f"{period-1}_CurrentActualUPB"] - df[f"{period}_CurrentActualUPB"]
        features[f"principal_reduction_{period}"] = reduction
    
    # Velocity metrics
    reduction_cols = [col for col in features.columns if col.startswith('principal_reduction_')]
    features['avg_principal_reduction'] = features[reduction_cols].mean(axis=1)
    features['total_principal_reduced'] = features[reduction_cols].sum(axis=1)
    features['std_principal_reduction'] = features[reduction_cols].std(axis=1)
    
    # Acceleration (is payment speed increasing or decreasing?)
    early_reduction = features[[f"principal_reduction_{i}" for i in range(1, 7)]].mean(axis=1)
    late_reduction = features[[f"principal_reduction_{i}" for i in range(7, 14)]].mean(axis=1)
    features['payment_acceleration'] = late_reduction - early_reduction
    
    # Expected vs actual reduction
    expected_reduction = df['OriginalUPB'] - df['13_CurrentActualUPB']
    features['expected_vs_actual_reduction'] = features['total_principal_reduced'] / expected_reduction.replace(0, np.nan)
    
    return features

# ============================================================================
# FEATURE GROUP 3: PAYMENT IRREGULARITY & STRESS SIGNALS
# Rationale: Inconsistent payments signal financial stress
# ============================================================================
def create_payment_irregularity_features(df):
    """Detect payment inconsistencies and stress signals"""
    features = pd.DataFrame(index=df.index)
    
    # Calculate scheduled payment
    scheduled_payment = df.apply(
        lambda x: calculate_monthly_payment(
            x['OriginalUPB'], 
            x['OriginalInterestRate'], 
            x['OriginalLoanTerm']
        ), 
        axis=1
    )
    
    # Track underpayments and missed payments
    underpayment_count = 0
    zero_payment_count = 0
    consecutive_underpayments = []
    
    for period in range(1, 14):
        actual_payment = df[f"{period-1}_CurrentActualUPB"] - df[f"{period}_CurrentActualUPB"]
        actual_payment = actual_payment.clip(lower=0)
        
        ratio = actual_payment / scheduled_payment.replace(0, np.nan)
        features[f"underpayment_{period}"] = (ratio < 0.9).astype(int)
        features[f"zero_payment_{period}"] = (actual_payment == 0).astype(int)
    
    # Count stress signals
    underpayment_cols = [col for col in features.columns if col.startswith('underpayment_')]
    zero_cols = [col for col in features.columns if col.startswith('zero_payment_')]
    
    features['total_underpayments'] = features[underpayment_cols].sum(axis=1)
    features['total_zero_payments'] = features[zero_cols].sum(axis=1)
    
    # Recent stress (last 3 months)
    features['recent_underpayments'] = features[[f"underpayment_{i}" for i in range(11, 14)]].sum(axis=1)
    features['recent_zero_payments'] = features[[f"zero_payment_{i}" for i in range(11, 14)]].sum(axis=1)
    
    # Longest consecutive underpayment streak
    for idx in df.index:
        max_streak = 0
        current_streak = 0
        for col in underpayment_cols:
            if features.loc[idx, col] == 1:
                current_streak += 1
                max_streak = max(max_streak, current_streak)
            else:
                current_streak = 0
        features.loc[idx, 'max_underpayment_streak'] = max_streak
    
    return features

# ============================================================================
# FEATURE GROUP 4: BALANCE TRAJECTORY & TREND ANALYSIS
# Rationale: Loan balance should decrease steadily; deviations are risky
# ============================================================================
def create_balance_trajectory_features(df):
    """Analyze how the loan balance changes over time"""
    features = pd.DataFrame(index=df.index)
    
    # Balance at each month
    balances = []
    for period in range(0, 14):
        balances.append(df[f"{period}_CurrentActualUPB"])
    balance_array = np.array(balances).T
    
    # Fit linear trend to balance over time
    time_points = np.arange(14)
    for idx in range(len(df)):
        slope, intercept = np.polyfit(time_points, balance_array[idx], 1)
        features.loc[df.index[idx], 'balance_trend_slope'] = slope
        features.loc[df.index[idx], 'balance_trend_intercept'] = intercept
    
    # Balance volatility
    features['balance_std'] = np.std(balance_array, axis=1)
    features['balance_cv'] = features['balance_std'] / np.mean(balance_array, axis=1)
    
    # Compare first vs last balance
    features['balance_pct_change'] = (df['13_CurrentActualUPB'] - df['0_CurrentActualUPB']) / df['0_CurrentActualUPB'].replace(0, np.nan)
    
    # Check if balance is stagnant (not decreasing)
    features['balance_stagnant'] = (features['balance_pct_change'] > -0.01).astype(int)
    
    # Remaining balance ratio
    features['remaining_balance_ratio'] = df['13_CurrentActualUPB'] / df['OriginalUPB']
    
    return features

# ============================================================================
# FEATURE GROUP 5: CREDIT RISK INDICATORS
# Rationale: Combine credit scores, DTI, LTV with payment behavior
# ============================================================================
def create_credit_risk_features(df):
    """Risk indicators from loan origination data"""
    features = pd.DataFrame(index=df.index)
    
    # Basic risk metrics
    features['credit_score'] = df['CreditScore']
    features['dti_ratio'] = df['OriginalDTI']
    features['ltv_ratio'] = df['OriginalLTV']
    features['cltv_ratio'] = df['OriginalCLTV']
    features['interest_rate'] = df['OriginalInterestRate']
    
    # Risk score (higher = riskier)
    features['risk_score'] = (
        (800 - df['CreditScore']) / 200 * 0.3 +  # Credit score component
        df['OriginalDTI'] / 50 * 0.3 +  # DTI component
        df['OriginalLTV'] / 100 * 0.2 +  # LTV component
        df['OriginalInterestRate'] / 10 * 0.2  # Interest rate component
    )
    
    # High-risk flags
    features['high_dti'] = (df['OriginalDTI'] > 43).astype(int)
    features['high_ltv'] = (df['OriginalLTV'] > 90).astype(int)
    features['low_credit'] = (df['CreditScore'] < 680).astype(int)
    
    # Mortgage insurance indicator
    features['has_mi'] = (df['MI_Pct'] > 0).astype(int)
    
    return features

# ============================================================================
# FEATURE GROUP 6: TEMPORAL & BEHAVIORAL CHANGES
# Rationale: Changes in payment behavior over time signal trouble
# ============================================================================
def create_temporal_change_features(df):
    """Detect changes in payment behavior over time"""
    features = pd.DataFrame(index=df.index)
    
    # Calculate payment ratios
    payment_ratios = []
    scheduled_payment = df.apply(
        lambda x: calculate_monthly_payment(
            x['OriginalUPB'], 
            x['OriginalInterestRate'], 
            x['OriginalLoanTerm']
        ), 
        axis=1
    )
    
    for period in range(1, 14):
        actual_payment = df[f"{period-1}_CurrentActualUPB"] - df[f"{period}_CurrentActualUPB"]
        actual_payment = actual_payment.clip(lower=0)
        ratio = actual_payment / scheduled_payment.replace(0, np.nan)
        payment_ratios.append(ratio)
    
    payment_ratio_array = np.array(payment_ratios).T
    
    # Early vs late period comparison
    early_avg = np.mean(payment_ratio_array[:, 0:6], axis=1)
    late_avg = np.mean(payment_ratio_array[:, 6:13], axis=1)
    
    features['early_payment_avg'] = early_avg
    features['late_payment_avg'] = late_avg
    features['payment_deterioration'] = early_avg - late_avg
    
    # Check for declining payment pattern
    features['payment_declining'] = (features['payment_deterioration'] > 0.1).astype(int)
    
    # Recent 3-month changes
    recent3_avg = np.mean(payment_ratio_array[:, -3:], axis=1)
    prev3_avg = np.mean(payment_ratio_array[:, -6:-3], axis=1)
    features['recent_payment_change'] = recent3_avg - prev3_avg
    
    # Volatility increase
    early_std = np.std(payment_ratio_array[:, 0:6], axis=1)
    late_std = np.std(payment_ratio_array[:, 6:13], axis=1)
    features['volatility_increase'] = late_std - early_std
    
    return features

# ============================================================================
# FEATURE GROUP 7: ADVANCED PAYMENT PATTERN ANALYSIS
# Rationale: Statistical patterns in payment behavior
# ============================================================================
def create_advanced_pattern_features(df):
    """Statistical analysis of payment patterns"""
    features = pd.DataFrame(index=df.index)
    
    # Calculate payment ratios
    payment_ratios = []
    scheduled_payment = df.apply(
        lambda x: calculate_monthly_payment(
            x['OriginalUPB'], 
            x['OriginalInterestRate'], 
            x['OriginalLoanTerm']
        ), 
        axis=1
    )
    
    for period in range(1, 14):
        actual_payment = df[f"{period-1}_CurrentActualUPB"] - df[f"{period}_CurrentActualUPB"]
        actual_payment = actual_payment.clip(lower=0)
        ratio = actual_payment / scheduled_payment.replace(0, np.nan)
        payment_ratios.append(ratio)
    
    payment_ratio_array = np.array(payment_ratios).T
    
    # Statistical moments
    features['payment_skewness'] = stats.skew(payment_ratio_array, axis=1)
    features['payment_kurtosis'] = stats.kurtosis(payment_ratio_array, axis=1)
    
    # Quartile-based features
    features['payment_q25'] = np.percentile(payment_ratio_array, 25, axis=1)
    features['payment_q75'] = np.percentile(payment_ratio_array, 75, axis=1)
    features['payment_iqr'] = features['payment_q75'] - features['payment_q25']
    
    # Count extreme values
    features['extreme_low_payments'] = (payment_ratio_array < 0.5).sum(axis=1)
    features['extreme_high_payments'] = (payment_ratio_array > 1.5).sum(axis=1)
    
    return features

# ============================================================================
# FEATURE TESTING AND SELECTION
# ============================================================================
def test_feature_group(X_train, X_valid, y_valid, feature_group_name, contamination=0.04):
    """Test a feature group with FULL ENSEMBLE (exactly like original script that got 0.418)"""
    print(f"\n  Testing: {feature_group_name}")
    print(f"  Features: {X_train.shape[1]}")
    
    # Train ALL models from original script
    models = {}
    
    # 1. LOF - n=35
    models['lof_35'] = LocalOutlierFactor(
        n_neighbors=35,
        contamination=contamination,
        novelty=True,
        n_jobs=-1
    )
    models['lof_35'].fit(X_train)
    
    # 2. One-Class SVM
    models['one_class_svm'] = OneClassSVM(
        kernel='rbf',
        gamma='auto',
        nu=min(contamination, 0.5)
    )
    models['one_class_svm'].fit(X_train)
    
    # 3. LOF - n=50
    models['lof_50'] = LocalOutlierFactor(
        n_neighbors=50,
        contamination=contamination,
        novelty=True,
        n_jobs=-1
    )
    models['lof_50'].fit(X_train)
    
    # PyOD models (ALL of them, exactly like original)
    if PYOD_AVAILABLE:
        try:
            models['ecod'] = ECOD(contamination=contamination)
            models['ecod'].fit(X_train)
        except:
            pass
        
        try:
            models['copod'] = COPOD(contamination=contamination)
            models['copod'].fit(X_train)
        except:
            pass
        
        try:
            models['hbos'] = HBOS(n_bins=10, contamination=contamination)
            models['hbos'].fit(X_train)
        except:
            pass
        
        try:
            models['knn_largest'] = KNN(
                n_neighbors=35,
                contamination=contamination,
                method='largest'
            )
            models['knn_largest'].fit(X_train)
        except:
            pass
        
        try:
            models['inne'] = INNE(
                n_estimators=100,
                max_samples='auto',
                contamination=contamination,
                random_state=42
            )
            models['inne'].fit(X_train)
        except:
            pass
        
        try:
            models['cblof'] = CBLOF(
                n_clusters=8,
                contamination=contamination,
                random_state=42
            )
            models['cblof'].fit(X_train)
        except:
            pass
        
        try:
            models['loda'] = LODA(n_bins=10, contamination=contamination)
            models['loda'].fit(X_train)
        except:
            pass
        
        try:
            models['cof'] = COF(n_neighbors=35, contamination=contamination)
            models['cof'].fit(X_train)
        except:
            pass
    
    # Generate predictions
    predictions = {}
    for name, model in models.items():
        try:
            if hasattr(model, 'decision_function'):
                scores = -model.decision_function(X_valid)
            elif hasattr(model, 'decision_scores_'):
                scores = model.decision_scores_
            else:
                continue
            predictions[name] = scores
        except:
            continue
    
    # Optimize ensemble weights
    best_weights = optimize_weights(predictions, y_valid)
    ensemble_scores = ensemble_predictions(predictions, best_weights)
    ap = average_precision_score(y_valid, ensemble_scores)
    
    print(f"  Models trained: {len(predictions)}")
    print(f"  Ensemble AP: {ap:.6f}")
    return ap, ensemble_scores

def ensemble_predictions(predictions, weights=None):
    """Ensemble predictions with optional weights"""
    if weights is None:
        weights = {name: 1/len(predictions) for name in predictions}
    
    normalized_preds = {}
    for name, scores in predictions.items():
        if len(scores) > 1:
            ranks = stats.rankdata(scores)
            normalized = ranks / len(ranks)
            normalized_preds[name] = normalized
        else:
            normalized_preds[name] = scores
    
    ensemble_scores = np.zeros(len(next(iter(normalized_preds.values()))))
    for name, scores in normalized_preds.items():
        ensemble_scores += weights[name] * scores
    
    if ensemble_scores.max() > ensemble_scores.min():
        ensemble_scores = (ensemble_scores - ensemble_scores.min()) / (ensemble_scores.max() - ensemble_scores.min())
    
    return ensemble_scores

def optimize_weights(predictions, y_true):
    """Optimize ensemble weights using greedy search"""
    best_ap = 0
    best_weights = {name: 1/len(predictions) for name in predictions}
    
    weight_options = np.arange(0.0, 1.05, 0.05)
    model_names = list(predictions.keys())
    
    if len(model_names) <= 2:
        for w1 in weight_options:
            weights = {model_names[0]: w1}
            if len(model_names) == 2:
                weights[model_names[1]] = 1 - w1
                if weights[model_names[1]] < 0:
                    continue
            
            ensemble_scores = ensemble_predictions(predictions, weights)
            ap = average_precision_score(y_true, ensemble_scores)
            
            if ap > best_ap:
                best_ap = ap
                best_weights = weights.copy()
    else:
        current_weights = {name: 1/len(predictions) for name in predictions}
        current_ap = average_precision_score(y_true, ensemble_predictions(predictions, current_weights))
        
        improved = True
        max_iterations = 100
        iteration = 0
        
        while improved and iteration < max_iterations:
            improved = False
            iteration += 1
            
            for model in model_names:
                for weight in weight_options:
                    test_weights = current_weights.copy()
                    test_weights[model] = weight
                    
                    weight_sum = sum(test_weights.values())
                    if weight_sum == 0:
                        continue
                    for m in model_names:
                        test_weights[m] /= weight_sum
                    
                    ensemble_scores = ensemble_predictions(predictions, test_weights)
                    ap = average_precision_score(y_true, ensemble_scores)
                    
                    if ap > current_ap + 1e-8:
                        current_ap = ap
                        current_weights = test_weights.copy()
                        improved = True
                        best_ap = ap
                        best_weights = test_weights.copy()
                
                if improved:
                    break
    
    return best_weights

def build_and_scale_features(df, feature_groups, is_train=True):
    """Build features from selected groups and scale them"""
    all_features = pd.DataFrame(index=df.index)
    
    for group_name, group_func in feature_groups:
        group_features = group_func(df)
        all_features = pd.concat([all_features, group_features], axis=1)
    
    numeric_cols = all_features.select_dtypes(include=np.number).columns
    
    if len(numeric_cols) > 0:
        if is_train:
            # Outlier clipping
            bounds = {}
            for col in numeric_cols:
                if all_features[col].notna().sum() > 0:
                    upper_bound = all_features[col].quantile(0.995)
                    lower_bound = all_features[col].quantile(0.005)
                    bounds[col] = {'upper': upper_bound, 'lower': lower_bound}
            globals()['_outlier_bounds'] = bounds
            
            for col in numeric_cols:
                if col in bounds:
                    all_features[col] = all_features[col].clip(
                        lower=bounds[col]['lower'], 
                        upper=bounds[col]['upper']
                    )
            
            # Imputation
            imputer = SimpleImputer(strategy='median')
            all_features[numeric_cols] = imputer.fit_transform(all_features[numeric_cols])
            globals()['_imputer'] = imputer
            
            # Scaling
            scaler = StandardScaler()
            all_features_scaled = pd.DataFrame(
                scaler.fit_transform(all_features),
                columns=all_features.columns,
                index=all_features.index
            )
            globals()['_scaler'] = scaler
        else:
            # Apply saved transformations
            if '_outlier_bounds' in globals():
                bounds = globals()['_outlier_bounds']
                for col in numeric_cols:
                    if col in bounds:
                        all_features[col] = all_features[col].clip(
                            lower=bounds[col]['lower'], 
                            upper=bounds[col]['upper']
                        )
            
            if '_imputer' in globals():
                imputer = globals()['_imputer']
                all_features[numeric_cols] = imputer.transform(all_features[numeric_cols])
            
            if '_scaler' in globals():
                scaler = globals()['_scaler']
                all_features_scaled = pd.DataFrame(
                    scaler.transform(all_features),
                    columns=all_features.columns,
                    index=all_features.index
                )
            else:
                all_features_scaled = all_features.copy()
    else:
        all_features_scaled = all_features
    
    return all_features_scaled

def main():
    print("="*80)
    print("ITERATIVE FEATURE ENGINEERING FOR LOAN DEFAULT DETECTION")
    print("="*80)
    print("Goal: Beat AP = 0.53335")
    print("Current Best: AP = 0.4182")
    print("="*80)
    
    print("\n[1/6] Loading data...")
    train_df, valid_df, test_df = load_data()
    y_valid = valid_df['target'].copy()
    print(f"  Train: {train_df.shape}, Valid: {valid_df.shape}, Test: {test_df.shape}")
    
    # Define feature groups
    feature_groups_catalog = {
        'baseline_payment': create_baseline_payment_features,
        'payment_velocity': create_payment_velocity_features,
        'payment_irregularity': create_payment_irregularity_features,
        'balance_trajectory': create_balance_trajectory_features,
        'credit_risk': create_credit_risk_features,
        'temporal_changes': create_temporal_change_features,
        'advanced_patterns': create_advanced_pattern_features,
    }
    
    print("\n[2/6] Testing individual feature groups...")
    print("="*80)
    
    group_scores = {}
    for group_name, group_func in feature_groups_catalog.items():
        X_train = build_and_scale_features(train_df, [(group_name, group_func)], is_train=True)
        X_valid = build_and_scale_features(valid_df, [(group_name, group_func)], is_train=False)
        
        ap, _ = test_feature_group(X_train, X_valid, y_valid, group_name)
        group_scores[group_name] = ap
    
    # Sort by performance
    sorted_groups = sorted(group_scores.items(), key=lambda x: x[1], reverse=True)
    print("\n" + "="*80)
    print("INDIVIDUAL GROUP RESULTS (sorted by AP):")
    for group_name, ap in sorted_groups:
        print(f"  {group_name:25s}: AP = {ap:.6f}")
    
    print("\n[3/6] Testing combinations of top feature groups...")
    print("="*80)
    
    # Test combinations starting with best group
    best_combination = []
    best_ap = 0
    best_X_train = None
    best_X_valid = None
    
    # Start with the best single group
    best_single_group = sorted_groups[0][0]
    best_combination.append((best_single_group, feature_groups_catalog[best_single_group]))
    best_ap = sorted_groups[0][1]
    
    # Build features for the best single group
    best_X_train = build_and_scale_features(train_df, best_combination, is_train=True)
    best_X_valid = build_and_scale_features(valid_df, best_combination, is_train=False)
    
    print(f"\nStarting with best group: {best_single_group} (AP={best_ap:.6f})")
    
    # Iteratively add groups that improve performance
    remaining_groups = [g for g in sorted_groups[1:]]
    
    for i, (group_name, _) in enumerate(remaining_groups):
        print(f"\n  Testing addition of: {group_name}")
        
        test_combination = best_combination + [(group_name, feature_groups_catalog[group_name])]
        X_train = build_and_scale_features(train_df, test_combination, is_train=True)
        X_valid = build_and_scale_features(valid_df, test_combination, is_train=False)
        
        ap, _ = test_feature_group(X_train, X_valid, y_valid, 
                                    f"{len(test_combination)} groups combined")
        
        if ap > best_ap:
            print(f"  ✓ Improved! {best_ap:.6f} → {ap:.6f} (+{ap-best_ap:.6f})")
            best_ap = ap
            best_combination = test_combination
            best_X_train = X_train
            best_X_valid = X_valid
        else:
            print(f"  ✗ No improvement: {ap:.6f} <= {best_ap:.6f}")
    
    print("\n" + "="*80)
    print(f"BEST COMBINATION: AP = {best_ap:.6f}")
    print("Feature groups included:")
    for group_name, _ in best_combination:
        print(f"  - {group_name}")
    print(f"Total features: {best_X_train.shape[1]}")
    
    print("\n[4/6] Testing different contamination values...")
    print("="*80)
    
    contamination_scores = {}
    contamination_values = [0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06]
    
    for contamination in contamination_values:
        ap, _ = test_feature_group(best_X_train, best_X_valid, y_valid, 
                                    f"contamination={contamination}", 
                                    contamination=contamination)
        contamination_scores[contamination] = ap
    
    best_contamination = max(contamination_scores, key=contamination_scores.get)
    best_final_ap = contamination_scores[best_contamination]
    
    print("\nContamination value results:")
    for cont, ap in sorted(contamination_scores.items()):
        marker = " ← BEST" if cont == best_contamination else ""
        print(f"  contamination={cont:.3f}: AP = {ap:.6f}{marker}")
    
    print("\n[5/6] Training final model with best configuration...")
    print("="*80)
    print(f"Best contamination: {best_contamination}")
    print(f"Best AP: {best_final_ap:.6f}")
    
    # Train ensemble with best configuration
    models = {}
    
    # LOF with optimal neighbors
    for n_neighbors in [25, 35, 50]:
        model = LocalOutlierFactor(
            n_neighbors=n_neighbors,
            contamination=best_contamination,
            novelty=True,
            n_jobs=-1
        )
        model.fit(best_X_train)
        models[f'lof_{n_neighbors}'] = model
    
    # One-Class SVM
    svm_model = OneClassSVM(
        kernel='rbf',
        gamma='auto',
        nu=min(best_contamination, 0.5)
    )
    svm_model.fit(best_X_train)
    models['one_class_svm'] = svm_model
    
    # PyOD models if available
    if PYOD_AVAILABLE:
        try:
            models['ecod'] = ECOD(contamination=best_contamination)
            models['ecod'].fit(best_X_train)
        except:
            pass
        
        try:
            models['copod'] = COPOD(contamination=best_contamination)
            models['copod'].fit(best_X_train)
        except:
            pass
    
    # Generate predictions
    valid_predictions = {}
    model_aps = {}
    for name, model in models.items():
        if hasattr(model, 'decision_function'):
            scores = -model.decision_function(best_X_valid)
        elif hasattr(model, 'decision_scores_'):
            scores = model.decision_scores_
        else:
            continue
        valid_predictions[name] = scores
        model_aps[name] = average_precision_score(y_valid, scores)
    
    print("\nIndividual model performance:")
    for name, ap in sorted(model_aps.items(), key=lambda x: x[1], reverse=True):
        print(f"  {name:20s}: AP = {ap:.6f}")
    
    print("\nOptimizing ensemble weights...")
    best_weights = optimize_weights(valid_predictions, y_valid)
    
    print("\nOptimal weights:")
    for name, weight in sorted(best_weights.items(), key=lambda x: x[1], reverse=True):
        if weight > 0.01:
            print(f"  {name:20s}: {weight:.4f}")
    
    # Weighted ensemble
    ensemble_scores = ensemble_predictions(valid_predictions, best_weights)
    ensemble_ap = average_precision_score(y_valid, ensemble_scores)
    
    print(f"\nWeighted Ensemble AP: {ensemble_ap:.6f}")
    print(f"Best single model AP: {max(model_aps.values()):.6f}")
    print(f"Ensemble improvement: +{ensemble_ap - max(model_aps.values()):.6f}")
    
    print("\n[6/6] Generating test predictions...")
    print("="*80)
    
    # Build test features
    X_test = build_and_scale_features(test_df, best_combination, is_train=False)
    
    # Generate test predictions
    test_predictions = {}
    for name, model in models.items():
        if hasattr(model, 'decision_function'):
            scores = -model.decision_function(X_test)
        elif hasattr(model, 'decision_scores_'):
            scores = model.decision_scores_
        else:
            continue
        test_predictions[name] = scores
    
    # Weighted ensemble using optimized weights
    test_ensemble_scores = ensemble_predictions(test_predictions, best_weights)
    test_probs = stats.beta.cdf(test_ensemble_scores, 0.5, 0.5)
    
    # Save results
    results = pd.DataFrame({
        'Id': test_df.index,
        'target': test_probs
    })
    
    output_filename = 'feature_engineered_predictions.csv'
    results.to_csv(output_filename, index=False)
    print(f"✓ Predictions saved to {output_filename}")
    
    # Save configuration
    config = {
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'final_ap': float(best_final_ap),
        'ensemble_ap': float(ensemble_ap),
        'model_aps': {k: float(v) for k, v in model_aps.items()},
        'feature_groups': [name for name, _ in best_combination],
        'n_features': int(best_X_train.shape[1]),
        'best_contamination': float(best_contamination),
        'best_weights': {k: float(v) for k, v in best_weights.items()},
        'group_scores': {k: float(v) for k, v in group_scores.items()},
        'contamination_scores': {str(k): float(v) for k, v in contamination_scores.items()}
    }
    
    config_filename = f"feature_engineering_config_{config['timestamp']}.json"
    with open(config_filename, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"✓ Configuration saved to {config_filename}")
    
    print("\n" + "="*80)
    print("FINAL RESULTS")
    print("="*80)
    print(f"Validation AP: {best_final_ap:.6f}")
    print(f"Target AP: 0.53335")
    print(f"Gap: {0.53335 - best_final_ap:.6f}")


if __name__ == "__main__":
    main()
